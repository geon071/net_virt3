# Домашнее задание к занятию 17 «Инцидент-менеджмент»

<details>
  <summary>Задание</summary>
Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.
</details>

## Ответ

|||
| ----------- | ----------- |
|Краткое описание инцидента|21 октября 2018 года, в следствии работ по тех. обсулуживанию, замена оптического оборудования, GitHub столкнулся с сбоем в работе своей системы, который привел к деградации сервиса на продолжительный период времени|
|Причина инцидента|Неверная, в рамках инфраструктуры, настройка репликатора MySQL orchestrator. Тригером инцидента послужила потеря связности между ядром сети и основным датацентром на восточном побережье США. Связность была восстановлена через 43 секунды, но это запустило цепочку событий, в результате которых деградировал кластер БД.|
|Воздействие|В течение 24 часов 11 минут все пользователи испытывали проблемы при работе: не работали Issues, Webhooks, невозможно было создать и разместить странички на GitHub Pages, на сайте отображалась устаревшая или неполная информация.|
|Обнаружение|В 22:54 внутренние системы мониторинга начали генерировать предупреждения о том, что в системе наблюдается множество сбоев.|
|Реакция|Дежурные инженеры остановили внутренний инструментарий развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений и перевели систему в статус "желтый". Позже был подключен координатор по инцидентам, который принял решение повысить статус сервиса до "красного". Подключили дополнительных инженеров по компетенции БД. Был сделан выбор приоритизировать сохранность данных перед сохранением функциональности сервиса, путем остановки веб-хуков и сборки GitHub Pages.|
|Восстановление|Инженеры разработали план по восстановлению основного кластера на восточном побережье из резервной копии и последующей репликации актуальных данных с кластера на западном побережье. В середине следующего рабочего дня, когда нагрузка на сервис стала пиковой, были развёрнуты дополнительные кластеры БД. На последнем этапе были возвращены в работу отключенные сервисы, и скопившиеся в беклоге задачи по вызову веб-хуков и публикации GitHub Pages начали выполняться, это заняло более 6 часов.|
|Таймлайн|<ul><li>2018 October 21 22:52 UTC: во время падения сети, развалился кластер БД, после восстановления связи в двух частях кластера оказались уникальные данные</li><li>2018 October 21 22:54 UTC: система мониторинга начала оповещать инженеров о многочисленных ошибках</li><li>2018 October 21 23:02 UTC: дежурные инженеры обнаружили что множество кластеров БД оказались в "неожиданном" состоянии</li><li>2018 October 21 23:07 UTC: команда залочила утилиты деплоя, для предотвращения дополнительных возможных изменений</li><li>2018 October 21 23:19 UTC: команда присвоила "желтый" статус системе для эскалации проблемы, координатору по инцидентам был отправлен алерт</li><li>2018 October 21 23:11 UTC: координатор подключился к команде, спустя две минуты изменил статус на "красный"</li><li>2018 October 21 23:13 UTC: подключили дополнительных инженеров из команды обслуживания БД</li><li>2018 October 21 23:19 UTC: инженеры намерено остановили работу веб-хуков и сборку GitHub Pages чтобы не подвергать данные пользователей дальнейшей опасности</li><li>2018 October 22 00:05 UTC: инженеры начали разработку плана по возврату кластера БД к консистентному состоянию, основная сложность была в объёме данных</li><li>2018 October 22 00:41 UTC: начался процесс восстановления из бекапа, параллельно инженеры искали способы ускорить передачу данных</li><li>2018 October 22 06:51 UTC: несколько кластеров завершили восстановление из бекапов в датацентре на восточном побережье и начали репликацию данных с датацентра на западном побережье</li><li>2018 October 22 07:46 UTC: GutHub опубликовали пост в блоге, чтобы донести пользователям больше информации. Это потребовало особых усилий, так блог использует GutHub Pages, сборка страниц для которого ранее была остановлена</li><li>2018 October 22 11:12 UTC: восстановился основной кластер БД на восточном побережье, что сделало систему гораздо более отзывчивой, т.к. сами приложения развёрнуты в том же датацентре. При этом множество read-only баз продолжали  репликацию, отставая от мастера на несколько часов, по этой причине множество пользователей получали неконсистентные данные. Стало понятно, что восстановление займёт больше времени, так как пользователи пришли в понедельник на работу и начали работу с сервисом, создавая новые данные.</li><li>2018 October 22 13:15 UTC: зафиксирован пик нагрузки трафика на GitHub.com, разрыв между мастером БД и репликами стал увеличиваться, вместо того чтобы сокращаться. Инженеры приняли решения развернуть дополнительные реплики MySQL на чтение.</li><li>2018 October 22 16:24 UTC: синхронизация реплик завершилась, инициирован возврат к исходной топологии</li><li>2018 October 22 16:45 UTC: началась обработка накопившегося стека событий, в беклоге были отложены миллион ивентов в хуках и 80 тысяч запросов на билд GitHub Pages</li><li>2018 October 22 23:03 UTC: работа веб-хуков и GutHub Pages восстановлена, связность всех систем подтверждена</li>|
|Последующие действия|<ul><li>Необходимо настроить Orchestrator таким образом, чтобы предотвратить передачу основных узлов базы данных через регионы.</li><li>Внедрен новый механизм отчетности о статусе, который предоставит более подробную информацию о текущих активных инцидентах.</li><li>Инвестирование в chaos engineering, тестируя различные сценарии отказа</li>|